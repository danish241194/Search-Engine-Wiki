{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 articles processed\n",
      "\n",
      "K - way Merging Start\n",
      "\n",
      "\n",
      "K - way Merging End\n",
      "\n",
      "Indexing Time :  0  hrs  14  mns 50  secs\n",
      "Total Articles : 19819\n"
     ]
    }
   ],
   "source": [
    "import xml.sax\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "files_to_index_at_a_time = 10000\n",
    "print_bool =False\n",
    "index_dictionary = {}\n",
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\"])\n",
    "EXTENDED_PUNCTUATIONS = set(list(string.punctuation) + ['\\n', '\\t', \" \"])\n",
    "INT_DIGITS = set([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"tempind_\")\n",
    "except:\n",
    "    pass\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'<(.*?)>','',text) #Remove tags if any\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE) #Remove Url\n",
    "    text = re.sub(r'{\\|(.*?)\\|}', '', text, flags=re.MULTILINE) #Remove CSS\n",
    "    text = re.sub(r'\\[\\[file:(.*?)\\]\\]', '', text, flags=re.MULTILINE) #Remove File\n",
    "    text = re.sub(r'[.,;_()\"/\\'=]', ' ', text, flags=re.MULTILINE) #Remove Punctuaion\n",
    "    text = re.sub(r'[~`!@#$%&-^*+{\\[}\\]()\":\\|\\\\<>/?]', ' ', text, flags=re.MULTILINE)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def get_InfoBox_Category_Text(body_text):\n",
    "    infoBox , category , body , links , references = [],[],[],[],[]\n",
    "    all_lines = body_text.split('\\n')\n",
    "    len_all_lines = len(all_lines)\n",
    "    i=0\n",
    "    \n",
    "    while i < len_all_lines:\n",
    "        if \"{{infobox\" in all_lines[i]:\n",
    "            open_curly_brackets = 0\n",
    "            while i < len_all_lines:\n",
    "                if \"{{\" in all_lines[i]:\n",
    "                    new_opened = all_lines[i].count(\"{{\")\n",
    "                    open_curly_brackets += new_opened\n",
    "                if \"}}\" in all_lines[i]:\n",
    "                    new_closed = all_lines[i].count(\"}}\")\n",
    "                    open_curly_brackets -= new_closed\n",
    "                if open_curly_brackets > 0:\n",
    "                        splitted_first_line = all_lines[i].split(\"{{infobox\");\n",
    "                        if(\"{{infobox\" in all_lines[i] and len(splitted_first_line) >= 2 and len(splitted_first_line[1])>0):\n",
    "                            infoBox.append(splitted_first_line[1])\n",
    "                        else :\n",
    "                            infoBox.append(all_lines[i])\n",
    "                else:\n",
    "                    break\n",
    "                i+=1\n",
    "        elif \"[[category:\" in all_lines[i]:\n",
    "            category_line_split = all_lines[i].split(\"[[category:\")\n",
    "            if(len(category_line_split)>1):\n",
    "                category.append(category_line_split[1].split(\"]]\")[0])\n",
    "                category.append(' ')\n",
    "        elif \"== external links ==\" in all_lines[i] or \"==external links ==\" in all_lines[i] or \"== external links==\" in all_lines[i] or \"==external links==\" in all_lines[i]:\n",
    "            i+=1\n",
    "            while i < len_all_lines:\n",
    "                if \"*[\" in all_lines[i] or \"* [\" in all_lines[i]:\n",
    "                    links.extend(all_lines[i].split(' '))\n",
    "                    i+=1\n",
    "                else:\n",
    "                    break \n",
    "        elif \"==references==\" in all_lines[i] or \"== references==\" in all_lines[i] or \"==references ==\" in all_lines[i] or \"== references ==\" in all_lines[i]:\n",
    "            open_curly_brackets = 0\n",
    "            i+=1\n",
    "            while i < len_all_lines:\n",
    "                if \"{{\" in all_lines[i]:\n",
    "                    new_opened = all_lines[i].count(\"{{\")\n",
    "                    open_curly_brackets += new_opened\n",
    "                if \"}}\" in all_lines[i]:\n",
    "                    new_closed = all_lines[i].count(\"}}\")\n",
    "                    open_curly_brackets -= new_closed\n",
    "                if open_curly_brackets > 0:\n",
    "                    if \"{{vcite\" not in all_lines[i] and \"{{cite\" not in all_lines[i] and \"{{reflist\" not in all_lines[i]:\n",
    "                        references.append(all_lines[i])\n",
    "                else:\n",
    "                    break\n",
    "                i+=1\n",
    "        else:\n",
    "            body.append(all_lines[i])\n",
    "        i+=1\n",
    "    return cleanText(''.join(infoBox)),cleanText(''.join(body)),cleanText(''.join(category)),cleanText(''.join(links)),cleanText(''.join(references))\n",
    "# stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def write_to_index(filenum,index_dictionary):\n",
    "    outF = open(\"tempind_/\"+str(filenum)+\".txt\", \"w\")\n",
    "    sorted_keys = sorted(index_dictionary.keys())\n",
    "    for key in sorted_keys:\n",
    "        outF.write(key+\":\"+process_line(key))\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()\n",
    "\n",
    "All_documents_done = True\n",
    "class Page:\n",
    "    def __init__(self):\n",
    "        self.title=\"\"\n",
    "        self.info = \"\"\n",
    "        self.category = \"\"\n",
    "        self.links = \"\"\n",
    "        self.references = \"\"\n",
    "        self.body = \"\"\n",
    "        self.pid = -1\n",
    "    def set_title(self,title):\n",
    "        self.title = title\n",
    "    def set_info_cat_links_ref_body(self,info,body,cat,links,ref):\n",
    "        self.body = body\n",
    "        self.info = info\n",
    "        self.category = cat\n",
    "        self.links = links\n",
    "        self.references = ref\n",
    "    def process(self):\n",
    "        if print_bool:\n",
    "            print(\"Page id \",self.pid)\n",
    "            print(\"TITLE \",self.title)\n",
    "            print(\"INFOBOX \",self.info)\n",
    "            print(\"CAT \",self.category)\n",
    "            print(\"LINKS \",self.links)\n",
    "            print(\"REFERENCES \",self.references)\n",
    "            print(\"BODY \",self.body)\n",
    "            print(\"\")\n",
    "       \n",
    "        self.Tokenize()\n",
    "        self.stop_word_removal()\n",
    "        self.Stemming()\n",
    "        self.create_index()\n",
    "    def Tokenize(self):\n",
    "        self.title = self.title.split()\n",
    "        self.info = self.info.split()\n",
    "        self.category = self.category.split()\n",
    "        self.links = self.links.split()\n",
    "        self.references = self.references.split()\n",
    "        self.body = self.body.split()\n",
    "        \n",
    "    def stop_word_removal(self):\n",
    "        self.title = [x for x in self.title if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.info = [x for x in self.info if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.category = [x for x in self.category if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.links = [x for x in self.links if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.references = [x for x in self.references if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.body = [x for x in self.body if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "\n",
    "    def Stemming(self):\n",
    "        self.title = [stemmer.stem(titl) for titl in self.title]\n",
    "        self.body = [stemmer.stem(titl) for titl in self.body]\n",
    "        self.references = [stemmer.stem(titl) for titl in self.references]\n",
    "        self.links = [stemmer.stem(titl) for titl in self.links]\n",
    "        self.category = [stemmer.stem(titl) for titl in self.category]\n",
    "        self.info = [stemmer.stem(titl) for titl in self.info]\n",
    "\n",
    "    def create_index(self):\n",
    "        final_dictionary = {}\n",
    "        dictionary_local = {}\n",
    "        title_split = self.title\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" t\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.body\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" b\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.info\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" i\"+str(dictionary_local[word])\n",
    "        \n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.category\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" c\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.links\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" l\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.references\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" r\"+str(dictionary_local[word])\n",
    "        for word in final_dictionary:\n",
    "            if index_dictionary.get(word) is None:\n",
    "                index_dictionary[word] = []\n",
    "            index_dictionary[word].append(final_dictionary[word])\n",
    "        dictionary_local.clear()\n",
    "        final_dictionary.clear()\n",
    "def process_line(key):\n",
    "    list_ = index_dictionary[key]\n",
    "    starter=\"\"\n",
    "    final_result=\"\"\n",
    "    for sub_list in list_:\n",
    "        sublist_split = sub_list.split(\" \");\n",
    "        final_result+=starter\n",
    "        is_page_number = True\n",
    "        for elem in sublist_split:\n",
    "            if is_page_number:\n",
    "                final_result+=elem+\"-\"\n",
    "                is_page_number=False\n",
    "            else:\n",
    "                final_result+=elem\n",
    "        starter=\"|\"\n",
    "    return final_result\n",
    "\n",
    "page = Page()\n",
    "title_pid=[]\n",
    "filenm=1\n",
    "\n",
    "def Kwaymerge():\n",
    "    import heapq\n",
    "    max_offset_file_size=10*1024*1024 #10 MB\n",
    "    offset_file_size = 0\n",
    "    dic_ = {}\n",
    "    file_num=1\n",
    "    heap = []\n",
    "    import os\n",
    "    num_files = len(os.listdir(\"tempind_\"))\n",
    "    while(file_num<=num_files):\n",
    "        fp = open('tempind_/'+str(file_num)+'.txt','r+')\n",
    "        heap.append((fp.readline().strip(),file_num))\n",
    "        dic_[file_num]=fp\n",
    "        file_num+=1\n",
    "    heapq.heapify(heap)\n",
    "    prev = \"....\"\n",
    "    outF = open(index_folder_path+\"/index1.txt\", \"w\")\n",
    "    outO = open(index_folder_path+\"/offset1.txt\", \"w\")\n",
    "    outS = open(index_folder_path+\"/secondary_index.txt\", \"w\")\n",
    "    First = True\n",
    "    offset= 0 \n",
    "    i_n = 2\n",
    "    while(len(heap)>0):\n",
    "        string = heap[0][0]\n",
    "        stream = dic_[heap[0][1]]\n",
    "        file_number = heap[0][1]\n",
    "        if string=='':\n",
    "            heapq.heappop(heap)\n",
    "            os.remove('tempind_/'+str(file_number)+'.txt')\n",
    "        else:\n",
    "            heapq.heappop(heap)\n",
    "            heapq.heappush(heap,(stream.readline().strip(),file_number))  \n",
    "            if string.split(\":\")[0] == prev:\n",
    "                outF.write(\"|\"+string.split(\":\")[1])\n",
    "                offset+=len(\"|\"+string.split(\":\")[1])\n",
    "            else:\n",
    "                if(offset_file_size>max_offset_file_size):\n",
    "                    prev = \"....\"\n",
    "                    outF.close()\n",
    "                    outO.close()\n",
    "                    outF = open(index_folder_path+\"/index\"+str(i_n)+\".txt\", \"w\")\n",
    "                    outO = open(index_folder_path+\"/offset\"+str(i_n)+\".txt\", \"w\")\n",
    "                    i_n+=1\n",
    "                    offset= 0 \n",
    "                    offset_file_size=0\n",
    "                    First = True\n",
    "                if First:\n",
    "                    outS.write(string.split(\":\")[0]+\" \"+str(i_n-1)+\"\\n\")\n",
    "                    First = False\n",
    "                else:\n",
    "                    offset+=1\n",
    "                    outF.write(\"\\n\")                 \n",
    "                prev = string.split(\":\")[0]\n",
    "                outO.write(string.split(\":\")[0]+\" \"+str(offset)+\"\\n\")\n",
    "                offset_file_size+=len(string.split(\":\")[0]+\" \"+str(offset)+\"\\n\")\n",
    "                outF.write(string)\n",
    "                offset += len(string)\n",
    "    outF.close()\n",
    "    outO.close()\n",
    "    outS.close()\n",
    "title_number=0\n",
    "outF_title = open(index_folder_path+\"/title\"+str(title_number)+\".txt\", \"w\")\n",
    "outF_offset = open(index_folder_path+\"/offset_title\"+str(title_number)+\".txt\", \"w\")\n",
    "offset_title=0\n",
    "class ParseHandler( xml.sax.ContentHandler ):\n",
    "    def __init__(self):\n",
    "        self.tag = \"\"\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.page = False\n",
    "    def startElement(self, tag, attributes):\n",
    "        global All_documents_done\n",
    "        self.tag = tag\n",
    "        if self.tag == \"page\":\n",
    "            self.page = True\n",
    "            All_documents_done = False\n",
    "            page.pid+=1            \n",
    "    def endElement(self, tag):\n",
    "        global filenm,All_documents_done,outF_title,title_number,offset_title,outF_offset\n",
    "        if tag==\"page\" and (page.pid+1)%files_to_index_at_a_time==0:\n",
    "            print(str(page.pid+1)+\" articles processed\")\n",
    "            write_to_index(filenm,index_dictionary)\n",
    "            index_dictionary.clear()\n",
    "            filenm=filenm+1\n",
    "            All_documents_done = True\n",
    "        if tag == \"page\":\n",
    "            self.page = False\n",
    "        elif tag == \"text\":\n",
    "            infobox , body , cat , links , ref = get_InfoBox_Category_Text(self.body.lower())\n",
    "            page.set_info_cat_links_ref_body(infobox,body,cat,links,ref)\n",
    "            page.process()\n",
    "            self.body = \"\"\n",
    "            \n",
    "        elif tag == \"title\":\n",
    "#             title_pid.append(self.title)\n",
    "            if (page.pid)%files_to_index_at_a_time==0:\n",
    "                outF_title.close()\n",
    "                outF_offset.close()\n",
    "                outF_offset = open(index_folder_path+\"/offset_title\"+str(title_number)+\".txt\", \"w\")\n",
    "                outF_title = open(index_folder_path+\"/title\"+str(title_number)+\".txt\", \"w\")\n",
    "                title_number+=1\n",
    "                offset_title=0\n",
    "            outF_offset.write(str(offset_title))\n",
    "            outF_offset.write(\"\\n\")\n",
    "            outF_title.write(self.title)\n",
    "            outF_title.write(\"\\n\")\n",
    "            offset_title+=len(self.title.encode('utf-8'))+1\n",
    "            \n",
    "\n",
    "            page.set_title(cleanText(''.join(self.title.lower())))\n",
    "        \n",
    "    def characters(self, content):\n",
    "        if self.page == True:    \n",
    "            if self.tag == \"title\":\n",
    "                self.title = content\n",
    "            elif self.tag == \"text\":\n",
    "                self.body +=content\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "Handler = ParseHandler()\n",
    "parser.setContentHandler( Handler )\n",
    "start = datetime.datetime.now()\n",
    "parser.parse(\"input_data/input.xml\")\n",
    "if not All_documents_done:\n",
    "    write_to_index(filenm,index_dictionary)\n",
    "    index_dictionary.clear()\n",
    "if not outF_title.closed:\n",
    "    outF_title.close()\n",
    "    outF_offset.close()\n",
    "index_folder_path = \"index_folder\"\n",
    "if index_folder_path[len(index_folder_path)-1]==\"/\":\n",
    "    index_folder_path = index_folder_path[:-1]\n",
    "    \n",
    "print()\n",
    "print(\"K - way Merging Start\")\n",
    "print()\n",
    "Kwaymerge()\n",
    "\n",
    "print()\n",
    "print(\"K - way Merging End\")\n",
    "print()\n",
    "end = datetime.datetime.now()\n",
    "secs  = (end-start).seconds\n",
    "hr = int(secs/(60*60))\n",
    "rm = int(secs%(60*60))\n",
    "mn = int(rm/60)\n",
    "rm=int(rm%60)\n",
    "secs = int(rm)\n",
    "print(\"Indexing Time : \",hr,\" hrs \",mn,\" mns\",secs,\" secs\")\n",
    "print(\"Total Articles : \"+str(page.pid+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Articles : 19819\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Articles : \"+str(page.pid+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'Jagiellonian University in Kraków'\n",
    "len(data)\n",
    "# data = data.encode('utf-8')\n",
    "# data.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import sys\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\"])\n",
    "Pstemmer = SnowballStemmer(\"english\")\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "class Query:\n",
    "    def __init__(self,query):\n",
    "        self.query = query\n",
    "    def Tokenize(self):\n",
    "        self.query = self.query.split()\n",
    "    def lower(self):\n",
    "        self.query = self.query.lower()\n",
    "    def stop_word_removal(self):\n",
    "        self.query = [x for x in self.query if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "    def Stemming(self):\n",
    "        self.query = [Pstemmer.stem(titl) for titl in self.query]\n",
    "    def process(self):\n",
    "        self.lower()\n",
    "        self.Tokenize()\n",
    "        self.stop_word_removal()\n",
    "        self.Stemming()\n",
    "    def value(self):\n",
    "        return self.query\n",
    "def title_for_docs(doc_ids,title_pid,k):\n",
    "    title = []\n",
    "    count = 0\n",
    "    for doc_id in doc_ids:\n",
    "        title.append(title_pid[int(doc_id)])\n",
    "        count+=1\n",
    "        if count==k:\n",
    "            break\n",
    "    return title\n",
    "def Search(query):\n",
    "    Q = Query(query)\n",
    "    Q.process()\n",
    "    query =  Q.value()\n",
    "    all_word_docs =[]\n",
    "    for word in query:\n",
    "        if word not in search_dictionary:\n",
    "            continue\n",
    "        line = search_dictionary[word].split(\"|\")\n",
    "        docs=set()\n",
    "        for doc in line:\n",
    "            docs.add(doc.split(\"-\")[0])\n",
    "        if(len(docs)>0):\n",
    "            all_word_docs.append(docs)\n",
    "    return sorted_results(all_word_docs)\n",
    "def sorted_results(all_word_docs):\n",
    "    if len(all_word_docs)==0:\n",
    "        return []\n",
    "    all_ = set.intersection(*all_word_docs)    \n",
    "    union = set.union(*all_word_docs)\n",
    "    diff = set.difference(union,all_)\n",
    "    all_=list(all_) + list(diff)\n",
    "    return all_\n",
    "def load_index_dictionary(path_to_index_folder):\n",
    "    dictionary_search = {}\n",
    "    fp = open(path_to_index_folder+\"/indexfile.txt\")\n",
    "    for i, line in enumerate(fp):#enumerate dont load whole in memory\n",
    "        word , rest = line.split(\":\")[0],line.split(\":\")[1][:-1]\n",
    "        dictionary_search[word] = rest\n",
    "    fp.close()\n",
    "    return dictionary_search\n",
    "def load_titles(path_to_index_folder):\n",
    "    titles = []\n",
    "    fp = open(path_to_index_folder+\"/title.txt\")\n",
    "    for i, line in enumerate(fp):#enumerate dont load whole in memory\n",
    "        titles.append(line[:-1])\n",
    "    fp.close()\n",
    "    return titles\n",
    "\n",
    "def read_file(testfile):\n",
    "    with open(testfile, 'r') as file:\n",
    "        queries = file.readlines()\n",
    "    return queries\n",
    "\n",
    "\n",
    "def write_file(outputs, path_to_output):\n",
    "    '''outputs should be a list of lists.\n",
    "        len(outputs) = number of queries\n",
    "        Each element in outputs should be a list of titles corresponding to a particular query.'''\n",
    "    with open(path_to_output, 'w') as file:\n",
    "        for output in outputs:\n",
    "            for line in output:\n",
    "                file.write(line.strip() + '\\n')\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "def search_help(path_to_index, queries):\n",
    "    '''Write your code here'''\n",
    "    title_pid = load_titles(path_to_index)\n",
    "    result = []\n",
    "    for query in queries:\n",
    "        if \":\" in query:\n",
    "            all_word_docs = Search2(query,search_dic)\n",
    "        else:\n",
    "            all_word_docs = Search(query)\n",
    "        result.append(title_for_docs(all_word_docs,title_pid,10))\n",
    "    return result\n",
    "\n",
    "def get_field_list(query):\n",
    "    query = query.replace(\"body:\",\"b:\").replace(\"title:\",\"t:\").replace(\"category:\",\"c:\").replace(\"infobox:\",\"i:\").replace(\"ref:\",\"e\")\n",
    "    words = query.split(\" \")\n",
    "    dictionary_query = {}\n",
    "    field = \"\"\n",
    "    for word in words:\n",
    "        if re.search(r'[t|b|c|e|i]{1,}:', word):\n",
    "            field = word.split(':')[0]\n",
    "            word = word.split(':')[1]\n",
    "        if field not in dictionary_query.keys():\n",
    "            dictionary_query[field] = []\n",
    "        dictionary_query[field].append(word)\n",
    "    return dictionary_query\n",
    "def page_number_for_field(search_dic,word,field_type):\n",
    "    result= []\n",
    "    if word not in search_dic:\n",
    "        return result\n",
    "    lst = search_dic[word].split(\"|\")\n",
    "    for l in lst:\n",
    "        if field_type in l:\n",
    "            result.append(l.split(\"-\")[0])\n",
    "    return result\n",
    "def Search2(query,search_dic):\n",
    "    query = query.lower()\n",
    "    field_dict = get_field_list(query)\n",
    "    field_results = []\n",
    "    for key in field_dict.keys():\n",
    "        key_list = []\n",
    "        lst = field_dict[key]\n",
    "        for word in lst:\n",
    "            word = Pstemmer.stem(word)\n",
    "            key_list = key_list + page_number_for_field(search_dic,word,key)\n",
    "        field_results.append(set(key_list))\n",
    "    return set.union(*field_results)\n",
    "\n",
    "def main():\n",
    "    path_to_index_folder = sys_argv[1]\n",
    "    if path_to_index_folder[len(path_to_index_folder)-1]==\"/\":\n",
    "        path_to_index_folder = path_to_index_folder[:-1]\n",
    "    testfile = sys_argv[2]\n",
    "    path_to_output = sys_argv[3]\n",
    "    queries = read_file(testfile)\n",
    "    outputs = search_help(path_to_index_folder, queries)\n",
    "    write_file(outputs, path_to_output)\n",
    "\n",
    "# sys_argv=[\"ss\",\"index_folder\",\"sample_queries.txt\",\"resultslog.txt\"]\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "def lower_bound(list_,word):\n",
    "    i = bisect.bisect_left(list_,word)\n",
    "    if(i<len(list_) and list_[i] == word):\n",
    "        return i\n",
    "    else:\n",
    "        return i-1\n",
    "def BinarySearch(list_,word): \n",
    "    i = bisect.bisect_left(list_,word) \n",
    "    if i != len(list_) and list_[i] == word: \n",
    "        return i \n",
    "    else: \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avicenna', 'Bandy']"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "import sys\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\"])\n",
    "Pstemmer = SnowballStemmer(\"english\")\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "class Query:\n",
    "    def __init__(self,query):\n",
    "        self.query = query\n",
    "    def Tokenize(self):\n",
    "        self.query = self.query.split()\n",
    "    def lower(self):\n",
    "        self.query = self.query.lower()\n",
    "    def stop_word_removal(self):\n",
    "        self.query = [x for x in self.query if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "    def Stemming(self):\n",
    "        self.query = [Pstemmer.stem(titl) for titl in self.query]\n",
    "    def process(self):\n",
    "        self.lower()\n",
    "        self.Tokenize()\n",
    "        self.stop_word_removal()\n",
    "        self.Stemming()\n",
    "    def value(self):\n",
    "        return self.query\n",
    "    \n",
    "secondary_list=[]\n",
    "with open(index_folder_path+'/secondary_index.txt') as f:\n",
    "    secondary_list= f.read().splitlines() \n",
    "    \n",
    "def get_posting_list(word):\n",
    "    global secondary_list\n",
    "    posting_list =  \"\"\n",
    "    offset_file_num = lower_bound(secondary_list,word)\n",
    "    if offset_file_num !=-1:\n",
    "        fp = open(index_folder_path+\"/offset\"+str(offset_file_num+1)+\".txt\")\n",
    "        dict_ = {}\n",
    "        while True:\n",
    "            string_ = fp.readline().strip();\n",
    "            if string_=='':\n",
    "                break;\n",
    "            dict_[string_.split(\" \")[0]]=int(string_.split(\" \")[1])\n",
    "        fp.close()\n",
    "        fp = open(index_folder_path+\"/index\"+str(offset_file_num+1)+\".txt\")\n",
    "        if word not in dict_:\n",
    "            return posting_list\n",
    "        fp.seek(dict_[word])\n",
    "        posting_list = fp.readline().strip().split(\":\")[1]\n",
    "        fp.close()\n",
    "    return posting_list\n",
    "def process_posting_field_tf(posting,field):\n",
    "    all_parts = posting.split(\"|\")\n",
    "    dict_={}\n",
    "    for part in all_parts:\n",
    "        doc = int(part.split(\"-\")[0])\n",
    "        stng = r\"\"+str(field)+'\\d*'\n",
    "        pattern = re.findall(stng,part.split(\"-\")[1])\n",
    "        sum_=0\n",
    "        if len(pattern)>0:\n",
    "            sum_ = int(pattern[0][1:])\n",
    "            dict_[doc]=sum_\n",
    "    return dict_\n",
    "def process_posting_idf(posting):\n",
    "    Total_documents = page.pid+1\n",
    "    return 1.0 + log(float(Total_documents) / len(posting.split(\"|\")))\n",
    "def process_posting_normal_tf(posting):\n",
    "    all_parts = posting.split(\"|\")\n",
    "    dict_={}\n",
    "    for part in all_parts:\n",
    "        doc = int(part.split(\"-\")[0])\n",
    "        pattern = re.findall(r'[a-z]\\d*',part.split(\"-\")[1])\n",
    "        sum_=0\n",
    "        for p in pattern:\n",
    "            sum_+=int(p[1:])\n",
    "        dict_[doc]=sum_\n",
    "    return dict_\n",
    "def calculate_tf_idf_of_docs_normal(query):\n",
    "    Q = Query(query)\n",
    "    Q.process()\n",
    "    query_parts =  Q.value()\n",
    "    docs = {}\n",
    "    for query_part in query_parts:\n",
    "        posting = get_posting_list(query_part)\n",
    "        if len(posting)<=0:\n",
    "            continue\n",
    "        dict_ = process_posting_normal_tf(posting)\n",
    "        idf = process_posting_idf(posting)\n",
    "        for key in dict_.keys():\n",
    "            try:\n",
    "                docs[key]+=log(1+dict_[key])*idf\n",
    "            except:\n",
    "                docs[key]=log(1+dict_[key])*idf\n",
    "    return docs\n",
    "def calculate_tf_idf_of_docs_field(query):\n",
    "    query = query.lower()\n",
    "    field_dict = get_field_list(query)\n",
    "    field_results = []\n",
    "    docs = {}\n",
    "    for key in field_dict.keys():\n",
    "        key_list = []\n",
    "        lst = field_dict[key]\n",
    "        for word in lst:\n",
    "            word = Pstemmer.stem(word)\n",
    "            posting = get_posting_list(word)\n",
    "            if(len(posting)<=0):\n",
    "                continue\n",
    "            dic_ = process_posting_field_tf(posting,key)\n",
    "            idf = process_posting_idf(posting)\n",
    "            for key_ in dic_.keys():\n",
    "                try:\n",
    "                    docs[key_]+=log(1+dict_[key])*idf\n",
    "                except:\n",
    "                    docs[key_]=log(1+dict_[key])*idf\n",
    "    return docs\n",
    "\n",
    "def get_field_list(query):\n",
    "    query = query.replace(\"body:\",\"b:\").replace(\"title:\",\"t:\").replace(\"category:\",\"c:\").replace(\"infobox:\",\"i:\").replace(\"ref:\",\"e\")\n",
    "    words = query.split(\" \")\n",
    "    dictionary_query = {}\n",
    "    field = \"\"\n",
    "    for word in words:\n",
    "        if re.search(r'[t|b|c|e|i]{1,}:', word):\n",
    "            field = word.split(':')[0]\n",
    "            word = word.split(':')[1]\n",
    "        if field not in dictionary_query.keys():\n",
    "            dictionary_query[field] = []\n",
    "        dictionary_query[field].append(word)\n",
    "    return dictionary_query\n",
    "\n",
    "def get_title_of_doc(doc_id):\n",
    "    file_num = int(doc_id/files_to_index_at_a_time)\n",
    "    line_num = int(doc_id%files_to_index_at_a_time)\n",
    "    with open(index_folder_path+\"/offset_title\"+str(file_num)+\".txt\") as f:\n",
    "        mylist = f.read().splitlines()    \n",
    "    title_file = open(index_folder_path+\"/title\"+str(file_num)+\".txt\",'r')\n",
    "    title_file.seek(int(mylist[line_num]))\n",
    "    result  = title_file.readline().strip()\n",
    "\n",
    "    title_file.close()\n",
    "    return result\n",
    "\n",
    "def get_titles(doc_ids):\n",
    "    titles=[]\n",
    "    for doc_id in doc_ids:\n",
    "        if(len(titles)>10):\n",
    "            break\n",
    "        titles.append(get_title_of_doc(doc_id))\n",
    "    return titles\n",
    "def search_helper(query):\n",
    "    dict_={}\n",
    "    if \":\" in query:\n",
    "        dict_ = calculate_tf_idf_of_docs_field(query)\n",
    "    else:\n",
    "        dict_ = calculate_tf_idf_of_docs_normal(query)\n",
    "    sorted_x = sorted(dict_.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    return get_titles([a[0] for a in sorted_x])\n",
    "    \n",
    "search_helper(\"yunost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2738 John Calvin\n",
    "2750 Jagiellonian University in Kraków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 420 6787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Avicenna'"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_title_of_doc(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "title_file = open(\"/home/danish/Desktop/testfile.txt\",'r')\n",
    "title_file.seek(0)\n",
    "result  = title_file.readline()\n",
    "title_file.close()\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Jagiellonian University in Kraków'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.encode('utf-8')\n",
    "# s.decode('utf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = s.decode('utf-8')\n",
    "len(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
