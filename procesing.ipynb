{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 articles processed\n",
      "100000 articles processed\n",
      "150000 articles processed\n",
      "200000 articles processed\n",
      "250000 articles processed\n",
      "300000 articles processed\n",
      "350000 articles processed\n",
      "400000 articles processed\n",
      "450000 articles processed\n",
      "500000 articles processed\n",
      "550000 articles processed\n",
      "600000 articles processed\n",
      "650000 articles processed\n",
      "700000 articles processed\n",
      "750000 articles processed\n",
      "800000 articles processed\n",
      "850000 articles processed\n",
      "900000 articles processed\n",
      "950000 articles processed\n",
      "1000000 articles processed\n",
      "1050000 articles processed\n",
      "1100000 articles processed\n",
      "1150000 articles processed\n",
      "1200000 articles processed\n",
      "1250000 articles processed\n",
      "1300000 articles processed\n",
      "1350000 articles processed\n",
      "1400000 articles processed\n",
      "1450000 articles processed\n",
      "1500000 articles processed\n",
      "1550000 articles processed\n",
      "1600000 articles processed\n",
      "1650000 articles processed\n",
      "1700000 articles processed\n",
      "1750000 articles processed\n",
      "1800000 articles processed\n",
      "1850000 articles processed\n",
      "1900000 articles processed\n",
      "1950000 articles processed\n",
      "2000000 articles processed\n",
      "2050000 articles processed\n",
      "2100000 articles processed\n",
      "2150000 articles processed\n",
      "2200000 articles processed\n",
      "2250000 articles processed\n",
      "2300000 articles processed\n",
      "2350000 articles processed\n",
      "2400000 articles processed\n",
      "2450000 articles processed\n",
      "2500000 articles processed\n",
      "2550000 articles processed\n",
      "2600000 articles processed\n",
      "2650000 articles processed\n",
      "2700000 articles processed\n",
      "2750000 articles processed\n",
      "2800000 articles processed\n",
      "2850000 articles processed\n",
      "2900000 articles processed\n",
      "2950000 articles processed\n",
      "3000000 articles processed\n",
      "3050000 articles processed\n",
      "3100000 articles processed\n",
      "3150000 articles processed\n",
      "3200000 articles processed\n",
      "3250000 articles processed\n",
      "3300000 articles processed\n",
      "3350000 articles processed\n",
      "3400000 articles processed\n",
      "3450000 articles processed\n",
      "3500000 articles processed\n",
      "3550000 articles processed\n",
      "3600000 articles processed\n",
      "3650000 articles processed\n",
      "3700000 articles processed\n",
      "3750000 articles processed\n",
      "3800000 articles processed\n",
      "3850000 articles processed\n",
      "3900000 articles processed\n",
      "3950000 articles processed\n",
      "4000000 articles processed\n",
      "4050000 articles processed\n",
      "4100000 articles processed\n",
      "4150000 articles processed\n",
      "4200000 articles processed\n",
      "4250000 articles processed\n",
      "4300000 articles processed\n",
      "4350000 articles processed\n",
      "4400000 articles processed\n",
      "4450000 articles processed\n",
      "4500000 articles processed\n",
      "4550000 articles processed\n",
      "4600000 articles processed\n",
      "4650000 articles processed\n",
      "4700000 articles processed\n",
      "4750000 articles processed\n",
      "4800000 articles processed\n",
      "4850000 articles processed\n",
      "4900000 articles processed\n",
      "4950000 articles processed\n",
      "5000000 articles processed\n",
      "5050000 articles processed\n",
      "5100000 articles processed\n",
      "5150000 articles processed\n",
      "5200000 articles processed\n",
      "5250000 articles processed\n",
      "5300000 articles processed\n",
      "5350000 articles processed\n",
      "5400000 articles processed\n",
      "5450000 articles processed\n",
      "5500000 articles processed\n",
      "5550000 articles processed\n",
      "5600000 articles processed\n",
      "5650000 articles processed\n",
      "5700000 articles processed\n",
      "5750000 articles processed\n",
      "5800000 articles processed\n",
      "5850000 articles processed\n",
      "5900000 articles processed\n",
      "5950000 articles processed\n",
      "6000000 articles processed\n",
      "6050000 articles processed\n",
      "6100000 articles processed\n",
      "6150000 articles processed\n",
      "6200000 articles processed\n",
      "6250000 articles processed\n",
      "6300000 articles processed\n",
      "6350000 articles processed\n",
      "6400000 articles processed\n",
      "6450000 articles processed\n",
      "6500000 articles processed\n",
      "6550000 articles processed\n",
      "6600000 articles processed\n",
      "6650000 articles processed\n",
      "6700000 articles processed\n",
      "6750000 articles processed\n",
      "6800000 articles processed\n",
      "6850000 articles processed\n",
      "6900000 articles processed\n",
      "6950000 articles processed\n",
      "7000000 articles processed\n",
      "7050000 articles processed\n",
      "7100000 articles processed\n",
      "7150000 articles processed\n",
      "7200000 articles processed\n",
      "7250000 articles processed\n",
      "7300000 articles processed\n",
      "7350000 articles processed\n",
      "7400000 articles processed\n",
      "7450000 articles processed\n",
      "7500000 articles processed\n",
      "7550000 articles processed\n",
      "7600000 articles processed\n",
      "7650000 articles processed\n",
      "7700000 articles processed\n",
      "7750000 articles processed\n",
      "7800000 articles processed\n",
      "7850000 articles processed\n",
      "7900000 articles processed\n",
      "7950000 articles processed\n",
      "8000000 articles processed\n",
      "8050000 articles processed\n",
      "8100000 articles processed\n",
      "8150000 articles processed\n",
      "8200000 articles processed\n",
      "8250000 articles processed\n",
      "8300000 articles processed\n",
      "8350000 articles processed\n",
      "8400000 articles processed\n",
      "8450000 articles processed\n",
      "8500000 articles processed\n",
      "8550000 articles processed\n",
      "8600000 articles processed\n",
      "8650000 articles processed\n",
      "8700000 articles processed\n",
      "8750000 articles processed\n",
      "8800000 articles processed\n",
      "8850000 articles processed\n",
      "8900000 articles processed\n",
      "8950000 articles processed\n",
      "9000000 articles processed\n",
      "9050000 articles processed\n",
      "9100000 articles processed\n",
      "9150000 articles processed\n",
      "9200000 articles processed\n",
      "9250000 articles processed\n",
      "9300000 articles processed\n",
      "9350000 articles processed\n",
      "9400000 articles processed\n",
      "9450000 articles processed\n",
      "9500000 articles processed\n",
      "9550000 articles processed\n",
      "9600000 articles processed\n",
      "9650000 articles processed\n",
      "9700000 articles processed\n",
      "9750000 articles processed\n",
      "9800000 articles processed\n",
      "9850000 articles processed\n",
      "9900000 articles processed\n",
      "9950000 articles processed\n",
      "10000000 articles processed\n",
      "10050000 articles processed\n",
      "10100000 articles processed\n",
      "10150000 articles processed\n",
      "10200000 articles processed\n",
      "10250000 articles processed\n",
      "10300000 articles processed\n",
      "10350000 articles processed\n",
      "10400000 articles processed\n",
      "10450000 articles processed\n",
      "10500000 articles processed\n",
      "10550000 articles processed\n",
      "10600000 articles processed\n",
      "10650000 articles processed\n",
      "10700000 articles processed\n",
      "10750000 articles processed\n",
      "10800000 articles processed\n",
      "10850000 articles processed\n",
      "10900000 articles processed\n",
      "10950000 articles processed\n",
      "11000000 articles processed\n",
      "11050000 articles processed\n",
      "11100000 articles processed\n",
      "11150000 articles processed\n",
      "11200000 articles processed\n",
      "11250000 articles processed\n",
      "11300000 articles processed\n",
      "11350000 articles processed\n",
      "11400000 articles processed\n",
      "11450000 articles processed\n",
      "11500000 articles processed\n",
      "11550000 articles processed\n",
      "11600000 articles processed\n",
      "11650000 articles processed\n",
      "11700000 articles processed\n",
      "11750000 articles processed\n",
      "11800000 articles processed\n",
      "11850000 articles processed\n",
      "11900000 articles processed\n",
      "11950000 articles processed\n",
      "12000000 articles processed\n",
      "12050000 articles processed\n",
      "12100000 articles processed\n",
      "12150000 articles processed\n",
      "12200000 articles processed\n",
      "12250000 articles processed\n",
      "12300000 articles processed\n",
      "12350000 articles processed\n",
      "12400000 articles processed\n",
      "12450000 articles processed\n",
      "12500000 articles processed\n",
      "12550000 articles processed\n",
      "12600000 articles processed\n",
      "12650000 articles processed\n",
      "12700000 articles processed\n",
      "12750000 articles processed\n",
      "12800000 articles processed\n",
      "12850000 articles processed\n",
      "12900000 articles processed\n",
      "12950000 articles processed\n",
      "13000000 articles processed\n",
      "13050000 articles processed\n",
      "13100000 articles processed\n",
      "13150000 articles processed\n",
      "13200000 articles processed\n",
      "13250000 articles processed\n",
      "13300000 articles processed\n",
      "13350000 articles processed\n",
      "13400000 articles processed\n",
      "13450000 articles processed\n",
      "13500000 articles processed\n",
      "13550000 articles processed\n",
      "13600000 articles processed\n",
      "13650000 articles processed\n",
      "13700000 articles processed\n",
      "13750000 articles processed\n",
      "13800000 articles processed\n",
      "13850000 articles processed\n",
      "13900000 articles processed\n",
      "13950000 articles processed\n",
      "14000000 articles processed\n",
      "14050000 articles processed\n",
      "14100000 articles processed\n",
      "14150000 articles processed\n",
      "14200000 articles processed\n",
      "14250000 articles processed\n",
      "14300000 articles processed\n",
      "14350000 articles processed\n",
      "14400000 articles processed\n",
      "14450000 articles processed\n",
      "14500000 articles processed\n",
      "14550000 articles processed\n",
      "14600000 articles processed\n",
      "14650000 articles processed\n",
      "14700000 articles processed\n",
      "14750000 articles processed\n",
      "14800000 articles processed\n",
      "14850000 articles processed\n",
      "14900000 articles processed\n",
      "14950000 articles processed\n",
      "15000000 articles processed\n",
      "15050000 articles processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15100000 articles processed\n",
      "15150000 articles processed\n",
      "15200000 articles processed\n",
      "15250000 articles processed\n",
      "15300000 articles processed\n",
      "15350000 articles processed\n",
      "15400000 articles processed\n",
      "15450000 articles processed\n",
      "15500000 articles processed\n",
      "15550000 articles processed\n",
      "15600000 articles processed\n",
      "15650000 articles processed\n",
      "15700000 articles processed\n",
      "15750000 articles processed\n",
      "15800000 articles processed\n",
      "15850000 articles processed\n",
      "15900000 articles processed\n",
      "15950000 articles processed\n",
      "16000000 articles processed\n",
      "16050000 articles processed\n",
      "16100000 articles processed\n",
      "16150000 articles processed\n",
      "16200000 articles processed\n",
      "16250000 articles processed\n",
      "16300000 articles processed\n",
      "16350000 articles processed\n",
      "16400000 articles processed\n",
      "16450000 articles processed\n",
      "16500000 articles processed\n",
      "16550000 articles processed\n",
      "16600000 articles processed\n",
      "16650000 articles processed\n",
      "16700000 articles processed\n",
      "16750000 articles processed\n",
      "16800000 articles processed\n",
      "16850000 articles processed\n",
      "16900000 articles processed\n",
      "16950000 articles processed\n",
      "17000000 articles processed\n",
      "17050000 articles processed\n",
      "17100000 articles processed\n",
      "17150000 articles processed\n",
      "17200000 articles processed\n",
      "17250000 articles processed\n",
      "17300000 articles processed\n",
      "17350000 articles processed\n",
      "17400000 articles processed\n",
      "17450000 articles processed\n",
      "17500000 articles processed\n",
      "17550000 articles processed\n",
      "17600000 articles processed\n",
      "17650000 articles processed\n",
      "17700000 articles processed\n",
      "17750000 articles processed\n",
      "17800000 articles processed\n",
      "17850000 articles processed\n",
      "17900000 articles processed\n",
      "17950000 articles processed\n",
      "18000000 articles processed\n",
      "18050000 articles processed\n",
      "18100000 articles processed\n",
      "18150000 articles processed\n",
      "18200000 articles processed\n",
      "18250000 articles processed\n",
      "18300000 articles processed\n",
      "18350000 articles processed\n",
      "18400000 articles processed\n",
      "18450000 articles processed\n",
      "18500000 articles processed\n",
      "18550000 articles processed\n",
      "18600000 articles processed\n",
      "18650000 articles processed\n",
      "18700000 articles processed\n",
      "18750000 articles processed\n",
      "18800000 articles processed\n",
      "18850000 articles processed\n",
      "18900000 articles processed\n",
      "18950000 articles processed\n",
      "19000000 articles processed\n",
      "19050000 articles processed\n",
      "19100000 articles processed\n",
      "19150000 articles processed\n",
      "19200000 articles processed\n",
      "19250000 articles processed\n",
      "19300000 articles processed\n",
      "19350000 articles processed\n",
      "19400000 articles processed\n",
      "19450000 articles processed\n",
      "19500000 articles processed\n",
      "19550000 articles processed\n",
      "\n",
      "K - way Merging Start\n",
      "\n",
      "\n",
      "K - way Merging End\n",
      "\n",
      "Indexing Time :  23  hrs  10  mns 31  secs\n",
      "Total Articles : 19567269\n"
     ]
    }
   ],
   "source": [
    "import xml.sax\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "files_to_index_at_a_time = 50000\n",
    "print_bool =False\n",
    "index_dictionary = {}\n",
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\"])\n",
    "EXTENDED_PUNCTUATIONS = set(list(string.punctuation) + ['\\n', '\\t', \" \"])\n",
    "INT_DIGITS = set([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"tempind_\")\n",
    "except:\n",
    "    pass\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'<(.*?)>','',text) #Remove tags if any\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE) #Remove Url\n",
    "    text = re.sub(r'{\\|(.*?)\\|}', '', text, flags=re.MULTILINE) #Remove CSS\n",
    "    text = re.sub(r'\\[\\[file:(.*?)\\]\\]', '', text, flags=re.MULTILINE) #Remove File\n",
    "    text = re.sub(r'[.,;_()\"/\\'=]', ' ', text, flags=re.MULTILINE) #Remove Punctuaion\n",
    "    text = re.sub(r'[~`!@#$%&-^*+{\\[}\\]()\":\\|\\\\<>/?]', ' ', text, flags=re.MULTILINE)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def get_InfoBox_Category_Text(body_text):\n",
    "    infoBox , category , body , links , references = [],[],[],[],[]\n",
    "    all_lines = body_text.split('\\n')\n",
    "    len_all_lines = len(all_lines)\n",
    "    i=0\n",
    "    \n",
    "    while i < len_all_lines:\n",
    "        if \"{{infobox\" in all_lines[i]:\n",
    "            open_curly_brackets = 0\n",
    "            while i < len_all_lines:\n",
    "                if \"{{\" in all_lines[i]:\n",
    "                    new_opened = all_lines[i].count(\"{{\")\n",
    "                    open_curly_brackets += new_opened\n",
    "                if \"}}\" in all_lines[i]:\n",
    "                    new_closed = all_lines[i].count(\"}}\")\n",
    "                    open_curly_brackets -= new_closed\n",
    "                if open_curly_brackets > 0:\n",
    "                        splitted_first_line = all_lines[i].split(\"{{infobox\");\n",
    "                        if(\"{{infobox\" in all_lines[i] and len(splitted_first_line) >= 2 and len(splitted_first_line[1])>0):\n",
    "                            infoBox.append(splitted_first_line[1])\n",
    "                        else :\n",
    "                            infoBox.append(all_lines[i])\n",
    "                else:\n",
    "                    break\n",
    "                i+=1\n",
    "        elif \"[[category:\" in all_lines[i]:\n",
    "            category_line_split = all_lines[i].split(\"[[category:\")\n",
    "            if(len(category_line_split)>1):\n",
    "                category.append(category_line_split[1].split(\"]]\")[0])\n",
    "                category.append(' ')\n",
    "        elif \"== external links ==\" in all_lines[i] or \"==external links ==\" in all_lines[i] or \"== external links==\" in all_lines[i] or \"==external links==\" in all_lines[i]:\n",
    "            i+=1\n",
    "            while i < len_all_lines:\n",
    "                if \"*[\" in all_lines[i] or \"* [\" in all_lines[i]:\n",
    "                    links.extend(all_lines[i].split(' '))\n",
    "                    i+=1\n",
    "                else:\n",
    "                    break \n",
    "        elif \"==references==\" in all_lines[i] or \"== references==\" in all_lines[i] or \"==references ==\" in all_lines[i] or \"== references ==\" in all_lines[i]:\n",
    "            open_curly_brackets = 0\n",
    "            i+=1\n",
    "            while i < len_all_lines:\n",
    "                if \"{{\" in all_lines[i]:\n",
    "                    new_opened = all_lines[i].count(\"{{\")\n",
    "                    open_curly_brackets += new_opened\n",
    "                if \"}}\" in all_lines[i]:\n",
    "                    new_closed = all_lines[i].count(\"}}\")\n",
    "                    open_curly_brackets -= new_closed\n",
    "                if open_curly_brackets > 0:\n",
    "                    if \"{{vcite\" not in all_lines[i] and \"{{cite\" not in all_lines[i] and \"{{reflist\" not in all_lines[i]:\n",
    "                        references.append(all_lines[i])\n",
    "                else:\n",
    "                    break\n",
    "                i+=1\n",
    "        else:\n",
    "            body.append(all_lines[i])\n",
    "        i+=1\n",
    "    return cleanText(''.join(infoBox)),cleanText(''.join(body)),cleanText(''.join(category)),cleanText(''.join(links)),cleanText(''.join(references))\n",
    "# stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def write_to_index(filenum,index_dictionary):\n",
    "    outF = open(\"tempind_/\"+str(filenum)+\".txt\", \"w\")\n",
    "    sorted_keys = sorted(index_dictionary.keys())\n",
    "    for key in sorted_keys:\n",
    "        outF.write(key+\":\"+process_line(key))\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()\n",
    "\n",
    "All_documents_done = True\n",
    "class Page:\n",
    "    def __init__(self):\n",
    "        self.title=\"\"\n",
    "        self.info = \"\"\n",
    "        self.category = \"\"\n",
    "        self.links = \"\"\n",
    "        self.references = \"\"\n",
    "        self.body = \"\"\n",
    "        self.pid = -1\n",
    "    def set_title(self,title):\n",
    "        self.title = title\n",
    "    def set_info_cat_links_ref_body(self,info,body,cat,links,ref):\n",
    "        self.body = body\n",
    "        self.info = info\n",
    "        self.category = cat\n",
    "        self.links = links\n",
    "        self.references = ref\n",
    "    def process(self):\n",
    "        if print_bool:\n",
    "            print(\"Page id \",self.pid)\n",
    "            print(\"TITLE \",self.title)\n",
    "            print(\"INFOBOX \",self.info)\n",
    "            print(\"CAT \",self.category)\n",
    "            print(\"LINKS \",self.links)\n",
    "            print(\"REFERENCES \",self.references)\n",
    "            print(\"BODY \",self.body)\n",
    "            print(\"\")\n",
    "       \n",
    "        self.Tokenize()\n",
    "        self.stop_word_removal()\n",
    "        self.Stemming()\n",
    "        self.create_index()\n",
    "    def Tokenize(self):\n",
    "        self.title = self.title.split()\n",
    "        self.info = self.info.split()\n",
    "        self.category = self.category.split()\n",
    "        self.links = self.links.split()\n",
    "        self.references = self.references.split()\n",
    "        self.body = self.body.split()\n",
    "        \n",
    "    def stop_word_removal(self):\n",
    "        self.title = [x for x in self.title if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.info = [x for x in self.info if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.category = [x for x in self.category if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.links = [x for x in self.links if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.references = [x for x in self.references if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.body = [x for x in self.body if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "\n",
    "    def Stemming(self):\n",
    "        self.title = [stemmer.stem(titl) for titl in self.title]\n",
    "        self.body = [stemmer.stem(titl) for titl in self.body]\n",
    "        self.references = [stemmer.stem(titl) for titl in self.references]\n",
    "        self.links = [stemmer.stem(titl) for titl in self.links]\n",
    "        self.category = [stemmer.stem(titl) for titl in self.category]\n",
    "        self.info = [stemmer.stem(titl) for titl in self.info]\n",
    "\n",
    "    def create_index(self):\n",
    "        final_dictionary = {}\n",
    "        dictionary_local = {}\n",
    "        title_split = self.title\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" t\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.body\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" b\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.info\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" i\"+str(dictionary_local[word])\n",
    "        \n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.category\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" c\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.links\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" l\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.references\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" r\"+str(dictionary_local[word])\n",
    "        for word in final_dictionary:\n",
    "            if index_dictionary.get(word) is None:\n",
    "                index_dictionary[word] = []\n",
    "            index_dictionary[word].append(final_dictionary[word])\n",
    "        dictionary_local.clear()\n",
    "        final_dictionary.clear()\n",
    "def process_line(key):\n",
    "    list_ = index_dictionary[key]\n",
    "    starter=\"\"\n",
    "    final_result=\"\"\n",
    "    for sub_list in list_:\n",
    "        sublist_split = sub_list.split(\" \");\n",
    "        final_result+=starter\n",
    "        is_page_number = True\n",
    "        for elem in sublist_split:\n",
    "            if is_page_number:\n",
    "                final_result+=elem+\"-\"\n",
    "                is_page_number=False\n",
    "            else:\n",
    "                final_result+=elem\n",
    "        starter=\"|\"\n",
    "    return final_result\n",
    "\n",
    "page = Page()\n",
    "title_pid=[]\n",
    "filenm=1\n",
    "\n",
    "def Kwaymerge():\n",
    "    import heapq\n",
    "    max_offset_file_size=10*1024*1024 #10 MB\n",
    "    offset_file_size = 0\n",
    "    dic_ = {}\n",
    "    file_num=1\n",
    "    heap = []\n",
    "    import os\n",
    "    num_files = len(os.listdir(\"tempind_\"))\n",
    "    while(file_num<=num_files):\n",
    "        fp = open('tempind_/'+str(file_num)+'.txt','r+')\n",
    "        heap.append((fp.readline().strip(),file_num))\n",
    "        dic_[file_num]=fp\n",
    "        file_num+=1\n",
    "    heapq.heapify(heap)\n",
    "    prev = \"....\"\n",
    "    outF = open(index_folder_path+\"/index1.txt\", \"w\")\n",
    "    outO = open(index_folder_path+\"/offset1.txt\", \"w\")\n",
    "    outS = open(index_folder_path+\"/secondary_index.txt\", \"w\")\n",
    "    First = True\n",
    "    offset= 0 \n",
    "    i_n = 2\n",
    "    while(len(heap)>0):\n",
    "        string = heap[0][0]\n",
    "        stream = dic_[heap[0][1]]\n",
    "        file_number = heap[0][1]\n",
    "        if string=='':\n",
    "            heapq.heappop(heap)\n",
    "            os.remove('tempind_/'+str(file_number)+'.txt')\n",
    "        else:\n",
    "            heapq.heappop(heap)\n",
    "            heapq.heappush(heap,(stream.readline().strip(),file_number))  \n",
    "            if string.split(\":\")[0] == prev:\n",
    "                outF.write(\"|\"+string.split(\":\")[1])\n",
    "                offset+=len(\"|\"+string.split(\":\")[1])\n",
    "            else:\n",
    "                if(offset_file_size>max_offset_file_size):\n",
    "                    prev = \"....\"\n",
    "                    outF.close()\n",
    "                    outO.close()\n",
    "                    outF = open(index_folder_path+\"/index\"+str(i_n)+\".txt\", \"w\")\n",
    "                    outO = open(index_folder_path+\"/offset\"+str(i_n)+\".txt\", \"w\")\n",
    "                    i_n+=1\n",
    "                    offset= 0 \n",
    "                    offset_file_size=0\n",
    "                    First = True\n",
    "                if First:\n",
    "                    outS.write(string.split(\":\")[0]+\" \"+str(i_n-1)+\"\\n\")\n",
    "                    First = False\n",
    "                else:\n",
    "                    offset+=1\n",
    "                    outF.write(\"\\n\")                 \n",
    "                prev = string.split(\":\")[0]\n",
    "                outO.write(string.split(\":\")[0]+\" \"+str(offset)+\"\\n\")\n",
    "                offset_file_size+=len(string.split(\":\")[0]+\" \"+str(offset)+\"\\n\")\n",
    "                outF.write(string)\n",
    "                offset += len(string)\n",
    "    outF.close()\n",
    "    outO.close()\n",
    "    outS.close()\n",
    "title_number=0\n",
    "outF_title = open(index_folder_path+\"/title\"+str(title_number)+\".txt\", \"w\")\n",
    "outF_offset = open(index_folder_path+\"/offset_title\"+str(title_number)+\".txt\", \"w\")\n",
    "offset_title=0\n",
    "class ParseHandler( xml.sax.ContentHandler ):\n",
    "    def __init__(self):\n",
    "        self.tag = \"\"\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.page = False\n",
    "    def startElement(self, tag, attributes):\n",
    "        global All_documents_done\n",
    "        self.tag = tag\n",
    "        if self.tag == \"page\":\n",
    "            self.page = True\n",
    "            All_documents_done = False\n",
    "            page.pid+=1            \n",
    "    def endElement(self, tag):\n",
    "        global filenm,All_documents_done,outF_title,title_number,offset_title,outF_offset\n",
    "        if tag==\"page\" and (page.pid+1)%files_to_index_at_a_time==0:\n",
    "            print(str(page.pid+1)+\" articles processed\")\n",
    "            write_to_index(filenm,index_dictionary)\n",
    "            index_dictionary.clear()\n",
    "            filenm=filenm+1\n",
    "            All_documents_done = True\n",
    "        if tag == \"page\":\n",
    "            self.page = False\n",
    "        elif tag == \"text\":\n",
    "            infobox , body , cat , links , ref = get_InfoBox_Category_Text(self.body.lower())\n",
    "            page.set_info_cat_links_ref_body(infobox,body,cat,links,ref)\n",
    "            page.process()\n",
    "            self.body = \"\"\n",
    "            \n",
    "        elif tag == \"title\":\n",
    "#             title_pid.append(self.title)\n",
    "            if (page.pid)%files_to_index_at_a_time==0:\n",
    "                outF_title.close()\n",
    "                outF_offset.close()\n",
    "                outF_offset = open(index_folder_path+\"/offset_title\"+str(title_number)+\".txt\", \"w\")\n",
    "                outF_title = open(index_folder_path+\"/title\"+str(title_number)+\".txt\", \"w\")\n",
    "                title_number+=1\n",
    "                offset_title=0\n",
    "            outF_offset.write(str(offset_title))\n",
    "            outF_offset.write(\"\\n\")\n",
    "            outF_title.write(self.title)\n",
    "            outF_title.write(\"\\n\")\n",
    "            offset_title+=len(self.title.encode('utf-8'))+1\n",
    "            \n",
    "\n",
    "            page.set_title(cleanText(''.join(self.title.lower())))\n",
    "        \n",
    "    def characters(self, content):\n",
    "        if self.page == True:    \n",
    "            if self.tag == \"title\":\n",
    "                self.title = content\n",
    "            elif self.tag == \"text\":\n",
    "                self.body +=content\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "Handler = ParseHandler()\n",
    "parser.setContentHandler( Handler )\n",
    "start = datetime.datetime.now()\n",
    "parser.parse(\"input_data/input.xml\")\n",
    "if not All_documents_done:\n",
    "    write_to_index(filenm,index_dictionary)\n",
    "    index_dictionary.clear()\n",
    "if not outF_title.closed:\n",
    "    outF_title.close()\n",
    "    outF_offset.close()\n",
    "index_folder_path = \"index_folder\"\n",
    "if index_folder_path[len(index_folder_path)-1]==\"/\":\n",
    "    index_folder_path = index_folder_path[:-1]\n",
    "    \n",
    "print()\n",
    "print(\"K - way Merging Start\")\n",
    "print()\n",
    "Kwaymerge()\n",
    "\n",
    "print()\n",
    "print(\"K - way Merging End\")\n",
    "print()\n",
    "end = datetime.datetime.now()\n",
    "secs  = (end-start).seconds\n",
    "hr = int(secs/(60*60))\n",
    "rm = int(secs%(60*60))\n",
    "mn = int(rm/60)\n",
    "rm=int(rm%60)\n",
    "secs = int(rm)\n",
    "print(\"Indexing Time : \",hr,\" hrs \",mn,\" mns\",secs,\" secs\")\n",
    "print(\"Total Articles : \"+str(page.pid+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19567268"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "def lower_bound(list_,word):\n",
    "    i = bisect.bisect_left(list_,word)\n",
    "    if(i<len(list_) and list_[i] == word):\n",
    "        return i\n",
    "    else:\n",
    "        return i-1\n",
    "def BinarySearch(list_,word): \n",
    "    i = bisect.bisect_left(list_,word) \n",
    "    if i != len(list_) and list_[i] == word: \n",
    "        return i \n",
    "    else: \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Query : gandhi\n",
      "Results for Query \"gandhi\" in  0  seconds\n",
      "1. Mahatma Gandhi\n",
      "2. Indira Gandhi\n",
      "3. Assassination of Mahatma Gandhi\n",
      "4. Rajiv Gandhi\n",
      "5. Rahul Gandhi\n",
      "6. Wikipedia:WikiProject Spam/LinkReports/faculty.georgetown.edu\n",
      "7. Gandhism\n",
      "8. H. P. Gandhi\n",
      "9. Vallabhbhai Patel\n",
      "10. Salt March\n",
      "11. List of artistic depictions of Mahatma Gandhi\n",
      "\n",
      "Enter Query : exit\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import sys\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "import datetime\n",
    "files_to_index_at_a_time = 50000\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\"])\n",
    "Pstemmer = SnowballStemmer(\"english\")\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "class Query:\n",
    "    def __init__(self,query):\n",
    "        self.query = query\n",
    "    def Tokenize(self):\n",
    "        self.query = self.query.split()\n",
    "    def lower(self):\n",
    "        self.query = self.query.lower()\n",
    "    def stop_word_removal(self):\n",
    "        self.query = [x for x in self.query if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "    def Stemming(self):\n",
    "        self.query = [Pstemmer.stem(titl) for titl in self.query]\n",
    "    def process(self):\n",
    "        self.lower()\n",
    "        self.Tokenize()\n",
    "        self.stop_word_removal()\n",
    "        self.Stemming()\n",
    "    def value(self):\n",
    "        return self.query\n",
    "    \n",
    "secondary_list=[]\n",
    "with open(index_folder_path+'/secondary_index.txt') as f:\n",
    "    secondary_list= f.read().splitlines() \n",
    "    secondary_list = [a.split(\" \")[0] for a in secondary_list]\n",
    "#     print(secondary_list)\n",
    "def get_posting_list(word):\n",
    "    global secondary_list\n",
    "    posting_list =  \"\"\n",
    "    offset_file_num = lower_bound(secondary_list,word)\n",
    "#     print(\"o f n\",offset_file_num)\n",
    "    if offset_file_num !=-1:\n",
    "        fp = open(index_folder_path+\"/offset\"+str(offset_file_num+1)+\".txt\")\n",
    "        dict_ = {}\n",
    "        while True:\n",
    "            string_ = fp.readline().strip();\n",
    "            if string_=='':\n",
    "                break;\n",
    "            dict_[string_.split(\" \")[0]]=int(string_.split(\" \")[1])\n",
    "        fp.close()\n",
    "        fp = open(index_folder_path+\"/index\"+str(offset_file_num+1)+\".txt\")\n",
    "        if word not in dict_:\n",
    "            return posting_list\n",
    "        fp.seek(dict_[word])\n",
    "        posting_list = fp.readline().strip().split(\":\")[1]\n",
    "        fp.close()\n",
    "    return posting_list\n",
    "def process_posting_field_tf(posting,field):\n",
    "    all_parts = posting.split(\"|\")\n",
    "    dict_={}\n",
    "    for part in all_parts:\n",
    "        doc = int(part.split(\"-\")[0])\n",
    "        stng = r\"\"+str(field)+'\\d*'\n",
    "        pattern = re.findall(stng,part.split(\"-\")[1])\n",
    "        sum_=0\n",
    "        if len(pattern)>0:\n",
    "            sum_ = int(pattern[0][1:])\n",
    "            dict_[doc]=sum_\n",
    "    return dict_\n",
    "def process_posting_idf(posting):\n",
    "    Total_documents = 19567268+1#page.pid+1\n",
    "    return 1.0 + log(float(Total_documents) / len(posting.split(\"|\")))\n",
    "def process_posting_normal_tf(posting):\n",
    "    all_parts = posting.split(\"|\")\n",
    "    dict_={}\n",
    "    for part in all_parts:\n",
    "        doc = int(part.split(\"-\")[0])\n",
    "        pattern = re.findall(r'[a-z]\\d*',part.split(\"-\")[1])\n",
    "        sum_=0\n",
    "        for p in pattern:\n",
    "            sum_+=int(p[1:])\n",
    "        dict_[doc]=sum_\n",
    "    return dict_\n",
    "def calculate_tf_idf_of_docs_normal(query):\n",
    "    Q = Query(query)\n",
    "    Q.process()\n",
    "    query_parts =  Q.value()\n",
    "#     print(query_parts)\n",
    "    docs = {}\n",
    "    for query_part in query_parts:\n",
    "        posting = get_posting_list(query_part)\n",
    "#         print(posting)\n",
    "        if len(posting)<=0:\n",
    "            continue\n",
    "        dict_ = process_posting_normal_tf(posting)\n",
    "        idf = process_posting_idf(posting)\n",
    "        for key in dict_.keys():\n",
    "            try:\n",
    "                docs[key]+=log(1+dict_[key])*idf\n",
    "            except:\n",
    "                docs[key]=log(1+dict_[key])*idf\n",
    "    return docs\n",
    "\n",
    "def calculate_tf_idf_of_docs_field(query):\n",
    "    query = query.lower()\n",
    "    field_dict = get_field_list(query)\n",
    "    field_results = []\n",
    "    docs = {}\n",
    "    for key in field_dict.keys():\n",
    "        key_list = []\n",
    "        lst = field_dict[key]\n",
    "        for word in lst:\n",
    "            word = Pstemmer.stem(word)\n",
    "            posting = get_posting_list(word)\n",
    "            if(len(posting)<=0):\n",
    "                continue\n",
    "            dic_ = process_posting_field_tf(posting,key)\n",
    "            idf = process_posting_idf(posting)\n",
    "            for key_ in dic_.keys():\n",
    "                try:\n",
    "                    docs[key_]+=log(1+dic_[key_])*idf\n",
    "                except:\n",
    "                    docs[key_]=log(1+dic_[key_])*idf\n",
    "    return docs\n",
    "\n",
    "def get_field_list(query):\n",
    "    query = query.replace(\"body:\",\"b:\").replace(\"title:\",\"t:\").replace(\"category:\",\"c:\").replace(\"infobox:\",\"i:\").replace(\"ref:\",\"e\")\n",
    "    words = query.split(\" \")\n",
    "    dictionary_query = {}\n",
    "    field = \"\"\n",
    "    for word in words:\n",
    "        if re.search(r'[t|b|c|e|i]{1,}:', word):\n",
    "            field = word.split(':')[0]\n",
    "            word = word.split(':')[1]\n",
    "        if field not in dictionary_query.keys():\n",
    "            dictionary_query[field] = []\n",
    "        dictionary_query[field].append(word)\n",
    "    return dictionary_query\n",
    "\n",
    "def get_title_of_doc(doc_id):\n",
    "    file_num = int(doc_id/files_to_index_at_a_time)\n",
    "    line_num = int(doc_id%files_to_index_at_a_time)\n",
    "    with open(index_folder_path+\"/offset_title\"+str(file_num)+\".txt\") as f:\n",
    "        mylist = f.read().splitlines()    \n",
    "    title_file = open(index_folder_path+\"/title\"+str(file_num)+\".txt\",'r')\n",
    "    title_file.seek(int(mylist[line_num]))\n",
    "    result  = title_file.readline().strip()\n",
    "\n",
    "    title_file.close()\n",
    "    return result\n",
    "\n",
    "def get_titles(doc_ids):\n",
    "    titles=[]\n",
    "    for doc_id in doc_ids:\n",
    "        if(len(titles)>10):\n",
    "            break\n",
    "        titles.append(get_title_of_doc(doc_id))\n",
    "    return titles\n",
    "def search_helper(query):\n",
    "    dict_={}\n",
    "    if \":\" in query:\n",
    "        dict_ = calculate_tf_idf_of_docs_field(query)\n",
    "    else:\n",
    "        dict_ = calculate_tf_idf_of_docs_normal(query)\n",
    "    sorted_x = sorted(dict_.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    return get_titles([a[0] for a in sorted_x])\n",
    "cmd =\"cmd\"\n",
    "while cmd!=\"exit\":\n",
    "    cmd = input(\"\\nEnter Query : \")\n",
    "    if cmd==\"exit\":\n",
    "        continue\n",
    "    i=1\n",
    "    start_s = datetime.datetime.now()\n",
    "    res = search_helper(cmd)\n",
    "    end_s = datetime.datetime.now()\n",
    "    print(\"Results for Query \\\"\"+cmd+\"\\\" in \",(end_s - start_s).seconds,\" seconds\")\n",
    "    for r in res:\n",
    "        print(str(i)+\".\",r)\n",
    "        i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
