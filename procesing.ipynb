{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bool =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dictionary = {}\n",
    "STOPWORDS = set(stopwords.words('english')) \n",
    "URL_STOP_WORDS = set([\"http\", \"https\", \"www\", \"ftp\", \"com\", \"net\", \"org\", \"archives\", \"pdf\", \"html\", \"png\", \"txt\", \"redirect\"])\n",
    "EXTENDED_PUNCTUATIONS = set(list(string.punctuation) + ['\\n', '\\t', \" \"])\n",
    "INT_DIGITS = set([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = re.sub(r'<(.*?)>','',text) #Remove tags if any\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE) #Remove Url\n",
    "    text = re.sub(r'{\\|(.*?)\\|}', '', text, flags=re.MULTILINE) #Remove CSS\n",
    "    text = re.sub(r'\\[\\[file:(.*?)\\]\\]', '', text, flags=re.MULTILINE) #Remove File\n",
    "    text = re.sub(r'[.,;_()\"/\\'=]', ' ', text, flags=re.MULTILINE) #Remove Punctuaion\n",
    "    text = re.sub(r'[~`!@#$%&\\-^*+{\\[}\\]()\":\\|\\\\<>/?]', ' ', text, flags=re.MULTILINE)\n",
    "    return \" \".join(text.split())\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_InfoBox_Category_Text(body_text):\n",
    "    infoBox , category , body , links , references = [],[],[],[],[]\n",
    "    all_lines = body_text.split('\\n')\n",
    "    len_all_lines = len(all_lines)\n",
    "    i=0\n",
    "    \n",
    "    while i < len_all_lines:\n",
    "        if \"{{infobox\" in all_lines[i]:\n",
    "            open_curly_brackets = 0\n",
    "            while i < len_all_lines:\n",
    "                if \"{{\" in all_lines[i]:\n",
    "                    new_opened = all_lines[i].count(\"{{\")\n",
    "                    open_curly_brackets += new_opened\n",
    "                if \"}}\" in all_lines[i]:\n",
    "                    new_closed = all_lines[i].count(\"}}\")\n",
    "                    open_curly_brackets -= new_closed\n",
    "                if open_curly_brackets > 0:\n",
    "                        splitted_first_line = all_lines[i].split(\"{{infobox\");\n",
    "                        if(\"{{infobox\" in all_lines[i] and len(splitted_first_line) >= 2 and len(splitted_first_line[1])>0):\n",
    "                            infoBox.append(splitted_first_line[1])\n",
    "                        else :\n",
    "                            infoBox.append(all_lines[i])\n",
    "                else:\n",
    "                    break\n",
    "                i+=1\n",
    "        elif \"[[category:\" in all_lines[i]:\n",
    "            category_line_split = all_lines[i].split(\"[[category:\")\n",
    "            if(len(category_line_split)>1):\n",
    "                category.append(category_line_split[1].split(\"]]\")[0])\n",
    "                category.append(' ')\n",
    "        elif \"== external links ==\" in all_lines[i] or \"==external links ==\" in all_lines[i] or \"== external links==\" in all_lines[i] or \"==external links==\" in all_lines[i]:\n",
    "            i+=1\n",
    "            while i < len_all_lines:\n",
    "                if \"*[\" in all_lines[i] or \"* [\" in all_lines[i]:\n",
    "                    links.extend(all_lines[i].split(' '))\n",
    "                    i+=1\n",
    "                else:\n",
    "                    break \n",
    "        elif \"==references==\" in all_lines[i] or \"== references==\" in all_lines[i] or \"==references ==\" in all_lines[i] or \"== references ==\" in all_lines[i]:\n",
    "            open_curly_brackets = 0\n",
    "            i+=1\n",
    "            while i < len_all_lines:\n",
    "                if \"{{\" in all_lines[i]:\n",
    "                    new_opened = all_lines[i].count(\"{{\")\n",
    "                    open_curly_brackets += new_opened\n",
    "                if \"}}\" in all_lines[i]:\n",
    "                    new_closed = all_lines[i].count(\"}}\")\n",
    "                    open_curly_brackets -= new_closed\n",
    "                if open_curly_brackets > 0:\n",
    "                    if \"{{vcite\" not in all_lines[i] and \"{{cite\" not in all_lines[i] and \"{{reflist\" not in all_lines[i]:\n",
    "                        references.append(all_lines[i])\n",
    "                else:\n",
    "                    break\n",
    "                i+=1\n",
    "        else:\n",
    "            body.append(all_lines[i])\n",
    "        i+=1\n",
    "    return cleanText(''.join(infoBox)),cleanText(''.join(body)),cleanText(''.join(category)),cleanText(''.join(links)),cleanText(''.join(references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "class Page:\n",
    "    def __init__(self):\n",
    "        self.title=\"\"\n",
    "        self.info = \"\"\n",
    "        self.category = \"\"\n",
    "        self.links = \"\"\n",
    "        self.references = \"\"\n",
    "        self.body = \"\"\n",
    "        self.pid = -1\n",
    "    def set_title(self,title):\n",
    "        self.title = title\n",
    "    def set_info_cat_links_ref_body(self,info,body,cat,links,ref):\n",
    "        self.body = body\n",
    "        self.info = info\n",
    "        self.category = cat\n",
    "        self.links = links\n",
    "        self.references = ref\n",
    "    def process(self):\n",
    "        if print_bool:\n",
    "            print(\"Page id \",self.pid)\n",
    "            print(\"TITLE \",self.title)\n",
    "            print(\"INFOBOX \",self.info)\n",
    "            print(\"CAT \",self.category)\n",
    "            print(\"LINKS \",self.links)\n",
    "            print(\"REFERENCES \",self.references)\n",
    "            print(\"BODY \",self.body)\n",
    "            print(\"\")\n",
    "        self.Tokenize()\n",
    "        self.stop_word_removal()\n",
    "        self.Stemming()\n",
    "        self.create_index()\n",
    "    def Tokenize(self):\n",
    "        self.title = self.title.split()\n",
    "        self.info = self.info.split()\n",
    "        self.category = self.category.split()\n",
    "        self.links = self.links.split()\n",
    "        self.references = self.references.split()\n",
    "        self.body = self.body.split()\n",
    "        \n",
    "    def stop_word_removal(self):\n",
    "        self.title = [x for x in self.title if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.info = [x for x in self.info if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.category = [x for x in self.category if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.links = [x for x in self.links if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.references = [x for x in self.references if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "        self.body = [x for x in self.body if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "\n",
    "    def Stemming(self):\n",
    "        self.title = [stemmer.stem(titl) for titl in self.title]\n",
    "        self.body = [stemmer.stem(titl) for titl in self.body]\n",
    "        self.references = [stemmer.stem(titl) for titl in self.references]\n",
    "        self.links = [stemmer.stem(titl) for titl in self.links]\n",
    "        self.category = [stemmer.stem(titl) for titl in self.category]\n",
    "        self.info = [stemmer.stem(titl) for titl in self.info]\n",
    "\n",
    "    def create_index(self):\n",
    "        final_dictionary = {}\n",
    "        dictionary_local = {}\n",
    "        title_split = self.title\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"p\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" t\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.body\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"p\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" b\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.info\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"p\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" i\"+str(dictionary_local[word])\n",
    "        \n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.category\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"p\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" c\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.links\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"p\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" l\"+str(dictionary_local[word])\n",
    "        dictionary_local.clear()\n",
    "        dictionary_local = {}\n",
    "        title_split = self.references\n",
    "        for word in title_split:\n",
    "            if dictionary_local.get(word) is None:\n",
    "                dictionary_local[word] = 0\n",
    "            dictionary_local[word]+=1\n",
    "        for word in dictionary_local:\n",
    "            if final_dictionary.get(word) is None:\n",
    "                final_dictionary[word]=\"p\"+str(self.pid)\n",
    "            final_dictionary[word]+= \" r\"+str(dictionary_local[word])\n",
    "        for word in final_dictionary:\n",
    "            if index_dictionary.get(word) is None:\n",
    "                index_dictionary[word] = []\n",
    "            index_dictionary[word].append(final_dictionary[word])\n",
    "        dictionary_local.clear()\n",
    "        final_dictionary.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Creation Time 110  seconds\n"
     ]
    }
   ],
   "source": [
    "page = Page()\n",
    "title_pid=[]\n",
    "class ParseHandler( xml.sax.ContentHandler ):\n",
    "    def __init__(self):\n",
    "        self.tag = \"\"\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.page = False\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.tag = tag\n",
    "        if self.tag == \"page\":\n",
    "            self.page = True\n",
    "            page.pid+=1\n",
    "    def endElement(self, tag):\n",
    "        if self.tag == \"page\":\n",
    "            self.page = False\n",
    "        elif self.tag == \"text\":\n",
    "            infobox , body , cat , links , ref = get_InfoBox_Category_Text(self.body.lower())\n",
    "            page.set_info_cat_links_ref_body(infobox,body,cat,links,ref)\n",
    "            page.process()\n",
    "            self.body = \"\"\n",
    "        elif self.tag == \"title\":\n",
    "            title_pid.append(self.title)\n",
    "            page.set_title(cleanText(''.join(self.title.lower())))\n",
    "    def characters(self, content):\n",
    "        if self.page == True:    \n",
    "            if self.tag == \"title\":\n",
    "                self.title = content\n",
    "            elif self.tag == \"text\":\n",
    "                self.body +=content\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "Handler = ParseHandler()\n",
    "parser.setContentHandler( Handler )\n",
    "start = datetime.datetime.now()\n",
    "parser.parse(\"input_data/input.xml\")\n",
    "end = datetime.datetime.now()\n",
    "print(\"Index Creation Time\",(end-start).seconds,\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(key):\n",
    "    list_ = index_dictionary[key]\n",
    "    starter=\"\"\n",
    "    final_result=\"\"\n",
    "    for sub_list in list_:\n",
    "        sublist_split = sub_list.split(\" \");\n",
    "        final_result+=starter\n",
    "        is_page_number = True\n",
    "        for elem in sublist_split:\n",
    "            if is_page_number:\n",
    "                final_result+=elem+\"-\"\n",
    "                is_page_number=False\n",
    "            else:\n",
    "                final_result+=elem\n",
    "        starter=\"|\"\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(\"title.txt\", \"w\")\n",
    "for line in title_pid:\n",
    "    outF.write(line)\n",
    "    outF.write(\"\\n\")\n",
    "outF.close()\n",
    "    \n",
    "outF = open(\"myOutFile.txt\", \"w\")\n",
    "sorted_keys = sorted(index_dictionary.keys())\n",
    "for key in sorted_keys:\n",
    "    outF.write(key+\":\"+process_line(key))\n",
    "    outF.write(\"\\n\")\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kaand: Black Scandal', 'Template:Second Manmohan Singh Cabinet', 'Shashank Arora', 'National Institute of Biotechnology', 'Template:Railway stations in Gujarat']\n"
     ]
    }
   ],
   "source": [
    "class Query:\n",
    "    def __init__(self,query):\n",
    "        self.query = query\n",
    "    def Tokenize(self):\n",
    "        self.query = self.query.split()\n",
    "    def lower(self):\n",
    "        self.query = self.query.lower()\n",
    "    def stop_word_removal(self):\n",
    "        self.query = [x for x in self.query if x not in STOPWORDS and x not in URL_STOP_WORDS and isEnglish(x)]\n",
    "    def Stemming(self):\n",
    "        self.query = [Pstemmer.stem(titl) for titl in self.query]\n",
    "    def process(self):\n",
    "        self.lower()\n",
    "        self.Tokenize()\n",
    "        self.stop_word_removal()\n",
    "        self.Stemming()\n",
    "    def value(self):\n",
    "        return self.query\n",
    "def title_for_docs(doc_ids,title_pid,k):\n",
    "    title = []\n",
    "    count = 0\n",
    "    for doc_id in doc_ids:\n",
    "        title.append(title_pid[int(doc_id[1:])])\n",
    "        count+=1\n",
    "        if count==k:\n",
    "            break\n",
    "    return title\n",
    "def Search(query,search_dictionary):\n",
    "    Q = Query(query)\n",
    "    Q.process()\n",
    "    query =  Q.value()\n",
    "    all_word_docs =[]\n",
    "    for word in query:\n",
    "        if word not in search_dictionary:\n",
    "            continue\n",
    "        line = search_dictionary[word].split(\"|\")\n",
    "        docs=set()\n",
    "        for doc in line:\n",
    "            docs.add(doc.split(\"-\")[0])\n",
    "        if(len(docs)>0):\n",
    "            all_word_docs.append(docs)\n",
    "    return sorted_results(all_word_docs)\n",
    "def sorted_results(all_word_docs):\n",
    "    if len(all_word_docs)==0:\n",
    "        return []\n",
    "    all_ = set.intersection(*all_word_docs)    \n",
    "    union = set.union(*all_word_docs)\n",
    "    diff = set.difference(union,all_)\n",
    "    all_=list(all_) + list(diff)\n",
    "    return all_\n",
    "def load_index_dictionary():\n",
    "    dictionary_search = {}\n",
    "    fp = open(\"myOutFile.txt\")\n",
    "    for i, line in enumerate(fp):#enumerate dont load whole in memory\n",
    "        word , rest = line.split(\":\")[0],line.split(\":\")[1][:-1]\n",
    "        dictionary_search[word] = rest\n",
    "    fp.close()\n",
    "    return dictionary_search\n",
    "def load_titles():\n",
    "    titles = []\n",
    "    fp = open(\"title.txt\")\n",
    "    for i, line in enumerate(fp):#enumerate dont load whole in memory\n",
    "        titles.append(line[:-1])\n",
    "    fp.close()\n",
    "    return titles\n",
    "search_dic = load_index_dictionary()\n",
    "title_pid = load_titles()\n",
    "all_word_docs = Search(\"sachin tendulkar\",search_dic)\n",
    "print(title_for_docs(all_word_docs,title_pid,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
